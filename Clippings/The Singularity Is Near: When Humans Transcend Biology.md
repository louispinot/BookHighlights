#The Singularity Is Near: When Humans Transcend Biology
##Ray Kurzweil
-----------------------------

**1098 (highlight)**

Innovation is multiplicative, not additive. Technology, like any evolutionary process, builds on itself.


**1108 (highlight)**

Each stage of evolution builds on the fruits of the last stage, so the rate of progress of an evolutionary process increases at least exponentially over time.


**1110 (highlight)**

·An evolutionary process is not a closed system; evolution draws upon the chaos in the larger system in which it takes place


**1156 (highlight)**

S-curves are typical of biological growth: replication of a system of relatively fixed complexity (such as an organism of a particular species), operating in a competitive niche and struggling for finite local resources.


**1170 (highlight)**

In recent technological history, the invention of the computer initiated another surge, still ongoing, in the complexity of information that the human-machine civilization is capable of handling. This latter surge will not reach an asymptote until we saturate the matter and energy in our region of the universe with computation,


**1212 (highlight)**

The combination of amino acids into proteins and of nucleic acids into strings of RNA established the basic paradigm of biology. Strings of RNA (and later DNA) that self-replicated (Epoch Two) provided a digital method to record the results of evolutionary experiments.


**1355 (highlight)**

once nanotechnology-based manufacturing becomes a reality in about twenty years—is


**1381 (highlight)**

This remarkably smooth acceleration in price-performance of semiconductors has progressed through a series of stages of process technologies (defined by feature sizes) at ever smaller dimensions. The key feature size is now dipping below one hundred nanometers, which is considered the threshold of "nanotechnology."25


**1408 (highlight)**

Despite this massive deflation in the cost of information technologies, demand has more than kept up. The number of bits shipped has doubled every 1.1 years, faster than the halving time in cost per bit, which is 1.5 years.30 As a result, the semiconductor industry enjoyed 18 percent annual growth in total revenue from 1958 to 2002.31


**1412 (highlight)**

The share of value contributed by information technology for most categories of products and services is rapidly increasing. Even common manufactured products such as tables and chairs have an information content, represented by their computerized designs and the programming of the inventory-procurement systems and automated-fabrication systems used in their assembly.


**1433 (highlight)**

When Moore's Law reaches the end of its S-curve, now expected before 2020, the exponential growth will continue with three-dimensional molecular computing, which will constitute the sixth paradigm.


**1466 (highlight)**

As discussed above, Moore's Law narrowly refers to the number of transistors on an integrated circuit of fixed size and sometimes has been expressed even more narrowly in terms of transistor feature size. But the most appropriate measure to track price-performance is computational speed per unit cost, an index that takes into account many levels of "cleverness" (innovation, which is to say, technological evolution). In addition to all of the invention involved in integrated circuits, there are multiple layers of improvement in computer design (for example, pipelining, parallel processing, instruction look-ahead, instruction and memory caching, and many others).


**1475 (highlight)**

We might ask whether there are inherent limits to the capacity of matter and energy to support computational processes.


**1476 (highlight)**

we won't approach those limits until late in this century.


**1496 (highlight)**

        The future GNR (Genetics, Nanotechnology, Robotics) age (see chapter 5) will come about not from the exponential explosion of computation alone but rather from the interplay and myriad synergies that will result from multiple intertwined technological advances.


**1546 (highlight)**

rapidly growing interest in nanotechnology. Nanotechnology science citations have been increasing significantly over the past decade, as noted in the figure below.55         We see the same phenomenon in nanotechnology-related patents (below).56


**1550 (highlight)**

As we will explore in chapter 5, the genetics (or biotechnology) revolution is bringing the information revolution, with its exponentially increasing capacity and price-performance, to the field of biology. Similarly, the nanotechnology revolution will bring the rapidly increasing mastery of information to materials and mechanical systems. The robotics (or "strong AI") revolution involves the reverse engineering of the human brain, which means coming to understand human intelligence in information terms and then combining the resulting insights with increasingly powerful computational platforms. Thus, all three of the overlapping transformations—genetics, nanotechnology, and robotics—that will dominate the first half of this century represent different facets of the information revolution.


**1561 (highlight)**

Inherent in our expectation of a Singularity taking place in human history is the pervasive importance of information to the future of human experience.


**1562 (highlight)**

We see information at every level of existence. Every form of human knowledge and artistic expression—scientific and engineering ideas and designs, literature, music, pictures, movies—can be expressed as digital information.


**1574 (highlight)**

Physicist-mathematician Stephen Wolfram provides extensive evidence to show how increasing complexity can originate from a universe that is at its core a deterministic, algorithmic system


**1581 (highlight)**

Wolfram postulates that the universe itself is a giant cellular-automaton computer. In his hypothesis there is a digital basic for apparently analog phenomena (such as motion and time)


**1587 (highlight)**

Perhaps the first to postulate that the universe is being computed on a digital computer was Konrad Zuse in 1967.61


**1588 (highlight)**

An enthusiastic proponent of an information-based theory of physics was Edward Fredkin, who in the early 1980s proposed a "new theory of physics" founded on the idea that the universe is ultimately composed of software. We should not think of reality as consisting of particles and forces, according to Fredkin, but rather as bits of data modified according to computation rules.


**1615 (highlight)**

implies that information rather than matter and energy may be regarded as the more fundamental reality.65


**1660 (highlight)**

He makes the point that computation is essentially simple and ubiquitous. The repetitive application of simple computational transformations, according to Wolfram, is the true source of complexity in the world.


**1679 (highlight)**

Human beings fulfill a highly demanding purpose: they survive in a challenging ecological niche. Human beings represent an extremely intricate and elaborate hierarchy of other patterns. Wolfram regards any patterns that combine some recognizable features and unpredictable elements to be effectively equivalent to on another. But he does not show how a class 4 automaton can ever increase it complexity, let alone become a pattern as complex as a human being.         There is a missing link here, one that would account for how one gets from the interesting but ultimately routine patterns of a cellular automaton to the complexity of persisting structures that demonstrate higher levels of intelligence. For example, these class 4 patterns are not capable of solving interesting problems, and no amount of iteration moves them closer to doing so. Wolfram would counter than a rule 110 automaton could be used as a "universal computer."71 However, by itself, a universal computer is not capable of solving intelligent programs without what I would call "software." It is the complexity of the software that runs on a universal computer that is precisely the issue.


**1694 (highlight)**

Can We Evolve Artificial Intelligence from Simple Rules?


**1696 (highlight)**

One concept we need into consideration is conflict—that is, evolution.


**1700 (highlight)**

An evolutionary algorithm can start with randomly generated potential solutions to a problem, which are encoded in a digital genetic code. We then have the solutions compete with one another in a simulated evolutionary battle. The better solutions survive and procreate in a simulated sexual reproduction in which offspring solutions are created, drawing their genetic code (encoded solutions) from two parents. We can also introduce a rate of genetic mutation. Various high-level parameters of this process, such as the rate of mutation, the rate of offspring, and so on, are appropriately called "God parameters," and it is the job of the engineer designing the evolutionary algorithm to set them to reasonably optimal values. The process is run for many thousands of generations of simulated evolution, and at the end of the process one is likely to find solutions that are of a distinctly higher order than the starting ones.


**1707 (highlight)**

The results of these evolutionary (sometimes called genetic) algorithms can be elegant, beautiful, and intelligent solutions to complex problems.


**1709 (highlight)**

such as designing jet engines.


**1728 (highlight)**

However, we have the benefits of the billions of years of evolution that have already taken place, which are responsible for the greatly increased order of complexity in the natural world. We can now benefit from it by using out evolved tools to reverse engineer the products of biological evolution (most importantly, the human brain).


**1737 (highlight)**

computation is inherently simple: we can build any possible level of complexity from a foundation of the simplest possible manipulations of information.


**1750 (highlight)**

Although we need additional concepts to describe an evolutionary process that create intelligent solutions to problems, Wolfram's demonstration of the simplicity an ubiquity of computation is an important contribution in our understanding of the fundamental significance of information in the world.


**1765 (highlight)**

That may be so, but the digital divide is getting worse. RAY: I know that people keep saying that, but how can that possibly be true? The number of humans is growing only very slowly. The number of digitally connected humans, no matter how you measure it, is growing rapidly.


**1779 (highlight)**

And the time gap between leading and lagging edge is itself contracting. Right now I estimate this lag at about a decade. In a decade, it will be down to about half a decade.


**1794 (highlight)**

Contemporary economic theory and policy are based on outdated models that emphasize energy costs, commodity prices, and capital investment in plant and equipment as key driving factors, while largely overlooking computational capacity, memory, bandwidth, the size of technology, intellectual property, knowledge, and other increasingly vital (and increasingly increasing) constituents that are driving the economy.


**1799 (highlight)**

Economic imperative is the equivalent of survival in biological evolution.


**1814 (highlight)**

the actual adoption of these technologies progressed smoothly with no indication of boom or bust.


**1835 (highlight)**

In each case, the economy ends up exactly where it would have been had the recession/depression never occurred.


**1881 (note)**

up to where can the tech-induced drop in price-value offse the drop in employment and wages?


**1881 (highlight)**

these factors are offset by the exponential trends in the price-performance


**1884 (highlight)**

Since the information industry is becoming increasingly influential in all sectors of the economy, we are seeing the increasing impact of the IT industry's extraordinary deflation rates.


**1886 (highlight)**

Deflation during the Great Depression in the 1930s was due to a collapse of consumer confidence and a collapse of the money supply. Today's deflation is a completely different phenomenon, caused by rapidly increasing productivity and the increasing pervasiveness of information in all its forms.


**1891 (highlight)**

It is important to point out that a key implication of nanotechnology is that it will bring the economics of software to hardware—that is, to physical products.


**1916 (highlight)**

By the end of this decade, computers will disappear as distinct physical objects, with displays built in our eyeglasses, and electronics woven in our clothing, providing full-immersion visual virtual reality.


**1917 (highlight)**

Thus, "going to a Web site" will mean entering a virtual-reality environment—at least for the visual and auditory senses—where we can directly interact with products and people, both real and simulated. Although the simulated people will not be up to human standards—at least not by 2009—they will be quite satisfactory


**1939 (highlight)**

All of the technologies exhibiting exponential growth shown in the above charts are continuing without losing a beat through recent economic slowdowns. Market acceptance also shows no evidence of boom and bust.


**1940 (highlight)**

The overall growth of the economy reflects completely new forms and layers of wealth and value that did not previously exist, or at least that did not previously constitute a significant portion of the economy,


**1942 (note)**

quels mogens de financen pour ces nouveux modzleq economiques justemnt? la rev industrielle a eu les marches financiers, est ce que le finncement participatif sera le moyen de se financer dans ces nouvelles conditions? alors toutes ces startups de crowdfunding que je trouve manquer de differentiation les unes ac les autres seraient en fait des précurseurs  d'une vague bien plus large


**2026 (highlight)**

In the next section, I provide an analysis of the amount of computation and memory required to achieve human levels of intelligence and why we can be confident that these levels will be achieved in inexpensive computers within two decades.


**2028 (highlight)**

Even these very powerful computers will be far from optimal, and in the last section of this chapter I'll review the limits of computation according to the laws of physics as we understand them today. This will bring us to computers circa the late twenty-first century.


**2033 (highlight)**

Many of these independent technologies can be integrated into computational systems that will eventually approach the theoretical maximum capacity of matter and energy to perform computation and will far outpace the computational capacities of a human brain.


**2056 (highlight)**

Nanotube circuitry was controversial when I discussed it in 1999, but there has been dramatic progress in the technology over the past six years. Two major strides were made in 2001. A nanotube-based transistor (with dimensions of one by twenty nanometers), operating at room temperature and using only a single electron to switch between on and off states, was reported in the July 6, 2001, issue of Science.10 Around the same time, IBM also demonstrated an integrated circuit with one thousand nanotube-based transistors.11


**2062 (highlight)**

One of the challenges in using this technology is that some nanotubes are conductive (that is, simply transmit electricity), while others act like semiconductors (that is, are capable of switching and able to implement logic gates). The difference in capability is based on subtle structural features.


**2065 (highlight)**

The Berkeley and Stanford scientists addressed this issue by developing a fully automated method of sorting and discarding the nonsemiconductor nanotubes.


**2076 (highlight)**

The Nantero design provides random access as well as nonvolatility (data is retained when the power is off), meaning that it could potentially replace all of the primary forms of memory: RAM, flash, and disk.


**2078 (highlight)**

Computing with Molecules. In addition to nanotubes, major progress has been made in recent years in computing with just one or a few molecules.


**2086 (highlight)**

The one-terahertz speed predicted by Peter Burke for molecular circuits looks increasingly accurate, given the nanoscale transistor created by scientists at the University of Illinois at Urbana-Champaign. It runs at a frequency of 604 gigahertz (more than half a terahertz).16


**2088 (highlight)**

One type of molecule that researchers have found to have desirable properties for computing is called a "rotaxane," which can switch states by changing the energy level of a ringlike structure contained within the molecule. Rotaxane memory and electronic switching devices have been demonstrated, and they show the potential of storing one hundred gigabits (1011 bits) per square inch. The potential would be even greater if organized in three dimensions.


**2092 (highlight)**

Self-Assembly. Self-assembling of nanoscale circuits is another key enabling technique for effective nanoelectronics. Self-assembly allows improperly formed components to be discarded automatically and makes it possible for the potentially trillions of circuit components to organize themselves, rather than be painstakingly assembled in a top-down process.


**2095 (highlight)**

using chemistry rather than lithography,


**2107 (highlight)**

It's also important that nanocircuits be self-configuring. The large number of circuit components and their inherent fragility (due to their small size) make it inevitable that some portions of a circuit will not function correctly. It will not be economically feasible to discard an entire circuit simply because a small number of transistors out of a trillion are non functioning. To address this concern, future circuits will continuously monitor their own performance and route information around sections that are unreliable in the same manner that information on the Internet is routed around nonfunctioning nodes.


**2127 (highlight)**

Computing with DNA. DNA is nature's own nanoengineered computer, and its ability to store information and conduct logical manipulations at the molecular level has already been exploited in specialized "DNA computers."


**2140 (highlight)**

The key to the power of DNA computing is that it allows for testing each of the trillions of strands simultaneously.


**2143 (highlight)**

The Weizmann scientists demonstrated a configuration consisting of two spoonfuls of this liquid supercomputing system, which contained thirty million billion molecular computers and performed a total of 660 trillion calculations per second (6.6 Î 1014 cps). The energy consumption of these computers is extremely low, only fifty millionths of a watt for all thirty million billion computers.


**2146 (highlight)**

There's a limitation, however, to DNA computing: each of the many trillions of computers has to perform the same operation at the same time (although on different data), so that the device is a "single instruction multiple data" (SIMD) architecture. While there are important classes of problems that are amenable to a SIMD system (for example, processing every pixel in an image for image enhancement or compression, and solving combinatorial-logic problems), it is not possible to program them for general-purpose algorithms,


**2158 (highlight)**

The exciting property of spintronics is that no energy is required to change an electron's spin state.


**2163 (highlight)**

The potential, then, is to achieve the efficiencies of superconducting (that is, moving information at or close to the speed of light without any loss of information) at room temperature.


**2174 (highlight)**

display the magnetic properties needed for spintronics while still maintaining the crystalline structure silicon requires as a serniconductor.29


**2176 (highlight)**

The spin of an electron is a quantum property (subject to the laws of quantum mechanics), so perhaps the most important application of spintronics will be in quantum computing systems,


**2178 (highlight)**

        Spin has also been used to store information in the nucleus of atoms,


**2186 (highlight)**

SIMD technologies such as DNA computers and optical computers will have important specialized roles to play in the future of computation.


**2187 (highlight)**

The replication of certain aspects of the functionality of the human brain, such as processing sensory data, can use SIMD architectures. For other brain regions, such as those dealing with learning and reasoning, general-purpose computing with its "multiple instruction multiple data" (MIMD) architectures will be required.


**2189 (highlight)**

For high-performance MIMD computing, we will need to apply the three-dimensional molecular-computing paradigms described above.


**2197 (highlight)**

As with the DNA computer described above, a key to successful quantum computing is a careful statement of the problem, including a precise way to test possible answers. The quantum computer effectively tests every possible combination of values for the qubits. So a quantum computer with one thousand qubits would test 21,000


**2200 (highlight)**

A thousand-bit quantum computer would vastly outperform any conceivable DNA computer, or for that matter any conceivable nonquantum computer. There are two limitations to the process, however. The first is that, like the DNA and optical computers discussed above, only a special set of problems is amenable to being presented to a quantum computer.


**2206 (highlight)**

Interesting classes of problems amenable to quantum computing include breaking encryption codes


**2209 (highlight)**

In a conventional computer, it is a straightforward process to combine memory bits and logic gates. We cannot, however, create a twenty-qubit quantum computer simply by combining two ten-qubit machines. All of the qubits have to be quantum-entangled together, and that has proved to be challenging.


**2218 (highlight)**

The ultimate role of quantum computing remains unresolved. But even if a quantum computer with hundreds of entangled qubits proves feasible, it will remain a special-purpose device, although one with remarkable capabilities that cannot be emulated in any other way.


**2221 (highlight)**

molecular computing would be the sixth major computing paradigm, the idea was still controversial. There has been so much progress in the past five years that there has been a sea change in attitude among experts, and this is now a mainstream view. We already have proofs of concept for all of the major requirements for three-dimensional molecular computing: single-molecule transistors, memory cells based on atoms, nanowires, and methods to self-assemble and self-diagnose the trillions (potentially trillions of trillions) of components.


**2225 (highlight)**

Contemporary electronics proceeds from the design of detailed chip layouts to photolithography to the manufacturing of chips in large, centralized factories. Nanocircuits are more likely to be created in small chemistry flasks, a development that will be another important step in the decentralization of our industrial infrastructure and will maintain the law of accelerating returns through this century and beyond.


**2276 (highlight)**

it is clear that we can emulate the functionality of brain regions with less computation than would be required to simulate the precise nonlinear operation of each neuron and all of the neural components (that is, all of the complex interactions that take place inside each neuron). We come to the same conclusion when we attempt to simulate the functionality of organs in the body.


**2284 (highlight)**

Functional simulation of the brain is sufficient to re-create human powers of pattern recognition, intellect, and emotional intelligence. On the other hand, if we want to "upload" a particular person's personality


**2287 (highlight)**

then we may need to simulate neural processes at the level of individual neurons


**2293 (highlight)**

overall estimate of about 1019 cps for simulating the human brain at this level.41 We can therefore consider this an upper bound, but 1014 to 1016 cps to achieve functional equivalence of all brain regions is likely to be sufficient.


**2300 (highlight)**

In line with my earlier predictions, supercomputers will achieve my more conservative estimate of 1016 cps for functional human-brain emulation by early in the next decade


**2303 (highlight)**

Personal computers today provide more than 109 cps.


**2304 (note)**

figure for personal computers. 10^16 cps = upper estimate to simulate the human brain


**2304 (highlight)**

we will achieve 1016cps by 2025.


**2309 (highlight)**

We will also be able to amplify the power of personal computers by harvesting the unused computation power of devices on the Internet. New communication paradigms such as "mesh" computing contemplate treating every device in the network as a node rather than just a "spoke."43 In other words, instead of devices (such as personal computers and PDAs) merely sending information to and from nodes, each device will act as a node itself, sending information to and receiving information from every other device. That will create very robust, self-organizing communication networks.


**2317 (highlight)**

For these reasons, it is reasonable to expect human brain capacity, at least in terms of hardware computational capacity, for one thousand dollars by around 2020.


**2334 (highlight)**

According to the projections from the ITRS road map (see RAM chart on p. 57), we will be able to purchase 1013 bits of memory for one thousand dollars by around 2018. Keep in mind that this memory will be millions of times faster than the electrochemical memory process used in the human brain and thus will be far more effective.


**2340 (highlight)**

Based on the above analyses, it is reasonable to expect the hardware that can emulate human-brain functionality to be available for approximately one thousand dollars by around 2020. As we will discuss in chapter 4, the software that will replicate that functionality will take about a decade longer.


**2345 (highlight)**

biological neurons.         While human neurons are wondrous creations, we wouldn't (and don't) design computing circuits using the same slow methods. Despite the ingenuity of the designs evolved through natural selection, they are many orders of magnitude less capable than what we will be able to engineer. As we reverse engineer our bodies and brains, we will be in a position to create comparable systems that are far more durable and that operate thousands to millions of times faster


**2351 (highlight)**

        Most of the complexity of a human neuron is devoted to maintaining its life-support functions, not its information-processing capabilities.


**2361 (highlight)**

we might well wonder: are there ultimate limits to the capacity of matter and energy to perform computation? If so, what are these limits, and how long will it take to reach them?


**2364 (highlight)**

to consider the ultimate limits of computation is really to ask: what is the destiny of our civilization?


**2371 (highlight)**

A major factor in considering computational limits is the energy requirement. The energy required per MIPS for computing devices has been falling exponentially, as shown in the following figure.46


**2376 (highlight)**

Processor speed is related to voltage, and the power required is proportional to the square of the voltage. So running a processor at a slower speed significantly reduces power consumption. If we invest in more parallel processing rather than faster single processors, it is feasible for energy consumption and heat dissipation to keep pace with the growing MIPS per dollar, as the figure "Reduction in Watts per MIPS" shows.


**2379 (highlight)**

This is essentially the same solution that biological evolution developed in the design of animal brains. Human brains use about one hundred trillion computers (the interneuronal connections, where most of the processing takes place). But these processors are very low in computational power and therefore run relatively cool.


**2384 (highlight)**

putting multiple processors on a single chip. We will see chip technology move in this direction as a way of keeping power requirements and heat dissipation in check.47


**2392 (highlight)**

When a bit of information is erased, that information has to go somewhere. According to the laws of thermodynamics, the erased bit is essentially released into the surrounding environment, thereby increasing its entropy, which can be viewed as a measure of information (including apparently disordered information) in an environment.


**2398 (highlight)**

Rolf Landauer showed in 1961 that reversible logical operations such as NOT (turning a bit into its opposite) could be performed without putting energy in or taking heat out, but that irreversible logical operations such as AND (generating bit C, which is a 1 if and only if both inputs A and Bare 1) do require energy.48


**2402 (highlight)**

comprehensive review of the idea of reversible computing.50 The fundamental concept is that if you keep all the intermediate results and then run the algorithm backward when you've finished your calculation, you end up where you started, have used no energy, and generated no heat. Along the way, however, you've calculated the result of the algorithm.


**2413 (highlight)**

In terms of computation, and just considering the electromagnetic interactions, there are at least 1015 changes in state per bit per second going on inside a 2.2-pound rock, which effectively represents about 1042 (a million trillion trillion trillion) calculations per second. Yet the rock requires no energy input and generates no appreciable heat.


**2417 (highlight)**

Of course, despite all this activity at the atomic level, the rock is not performing any useful work aside from perhaps acting as a paperweight or a decoration. The reason for this is that the structure of the atoms in the rock is for the most part effectively random. If, on the other hand, we organize the particles in a more purposeful manner, we could have a cool, zero-energy-consuming computer


**2431 (highlight)**

Reversible logic has already been demonstrated and shows the expected reductions in energy input and heat dissipation.56 Fredkin's reversible logic gates answer a key challenge to the idea of reversible computing: that it would require a different style of programming. He argues that we can, in fact, construct normal logic and memory entirely from reversible logic gates, which will allow the use of existing conventional software-development methods.


**2441 (highlight)**

However, because of essentially random thermal and quantum effects, logic operations have an inherent error rate. We can overcome errors using error-detection and-correction codes, but each time we correct a bit, the operation is not reversible, which means it requires energy and generates heat. Generally, error rates are low. But even if errors occur at the rate of, say, one per 1010 operations, we have only succeeded in reducing energy requirements by a factor of 1010, not in eliminating energy dissipation altogether.


**2446 (highlight)**

As we consider the limits of computation, the issue of error rate becomes a significant design issue. Certain methods of increasing computational rate, such as increasing the frequency of the oscillation of particles, also increase error rates, so this puts natural limits on the ability to perform computation using matter and energy.


**2463 (highlight)**

        So there is a direct proportional relationship between the energy of an object and its potential to perform computation.


**2469 (highlight)**

Lloyd shows how the potential computing capacity of a kilogram of matter equals pi times energy divided by Planck's constant. Since the energy is such a large number and Planck's constant is so small, this equation generates an extremely large number: about 5 Î 1050 operations per second.59


**2478 (highlight)**

Again, a few caveats are in order. Converting all of the mass of our 2.2-pound laptop into energy is essentially what happens in a thermonuclear explosion.


**2479 (note)**

Lol, quel comique!


**2495 (highlight)**

Rather, technology will continue to ramp up, always using its latest prowess to progress to the next level. So once we get to a civilization with 1042 cps (for every 2.2 pounds), the scientists and engineers of that day will use their essentially vast nonbiological intelligence to figure out how to get 1043, then 1044, and so on. My expectation is that we will get very close to the ultimate limits.


**2522 (highlight)**

I set the date for the Singularity—representing a profound and disruptive transformation in human capability—as 2045.


**2524 (highlight)**

Despite the clear predominance of nonbiological intelligence by the mid-2040s, ours will still be a human civilization. We will transcend biology, but not our humanity.


**2531 (highlight)**

As we will see, the amount of time required for our human civilization to achieve scales of computation and intelligence that go beyond our planet and into the universe may be a lot shorter than you might think.


**2561 (highlight)**

what is the brain's computational efficiency?


**2562 (highlight)**

use the estimate of 1016 cps required to emulate the brain's functionality,


**2564 (highlight)**

With the theoretical capacity of the brain's atoms estimated at 1042 cps, this gives us a computational efficiency of 10–26. Again, that's closer to a rock than to the laptop, even on a logarithmic scale.         Our brains have evolved significantly in their memory and computational efficiency from pre-biology objects such as stones. But we clearly have many orders of magnitude of improvement to take advantage of during the first half of this century.


**2568 (highlight)**

The limits of around 1042 cps for a one-kilogram, one-liter cold computer and around 1050 for a (very) hot one are based on computing with atoms.


**2700 (highlight)**

Our ability to reverse engineer the brain—to see inside, model it, and simulate its regions—is growing exponentially. We will ultimately understand the principles of operation underlying the full range of our own thinking, knowledge that will provide us with powerful procedures for developing the software of intelligent machines.


**2704 (highlight)**

We will also gain powerful new ways to treat neurological problems such as Alzheimer's, stroke, Parkinson's disease, and sensory disabilities, and ultimately will be able to vastly extend our intelligence.


**2708 (highlight)**

significant number of new scanning technologies feature greatly improved spatial and temporal resolution, price-performance, and bandwidth.


**2716 (highlight)**

When we get to the nanobot era (see "Scanning Using Nanobots" on p. 163), we will be able to scan from inside the brain with exquisitely high spatial and temporal resolution.4


**2717 (highlight)**

There are no inherent barriers to our being able to reverse engineer the operating principles of human intelligence and replicate these capabilities in the more powerful computational substrates that will become available in the decades ahead.


**2722 (highlight)**

principal assumption underlying the expectation of the Singularity is that nonbiological mediums will be able to emulate the richness, subtlety, and depth of human thinking.


**2750 (highlight)**

design more intelligent parallel algorithms for our machines, particularly those based on self-organizing paradigms.         With this self-organizing approach, we don't have to attempt to replicate every single neural connection. There is a great deal of repetition and redundancy within any particular brain region. We are discovering that higher-level models of brain regions are often simpler than the detailed models of their neuronal components.


**2761 (highlight)**

Of course, the complexity of our brains greatly increases as we interact with the world (by a factor of about one billion over the genome).10


**2762 (highlight)**

But highly repetitive patterns are found in each specific brain region, so it is not necessary to capture each particular detail to successfully reverse engineer the relevant algorithms,


**2791 (highlight)**

Most brain-modeling algorithms are not the sequential, logical methods that are commonly used in digital computing today. The brain tends to use self-organizing, chaotic, holographic processes (that is, information not located in one place but distributed throughout a region). It is also massively parallel and utilizes hybrid digital-controlled analog techniques.


**2800 (highlight)**

The answer to this question depends on what we mean by the word "computer." Most computers today are all digital and perform one (or perhaps a few) computations at a time at extremely high speed. In contrast, the human brain combines digital and analog methods but performs most computations in the analog (continuous) domain, using neurotransmitters and related mechanisms. Although these neurons execute calculations at extremely slow speeds (typically two hundred transactions per second), the brain as a whole is massively parallel: most of its neurons work at the same time, resulting in up to one hundred trillion computations being carried out simultaneously.         The massive parallelism of the human brain is the key to its pattern-recognition ability, which is one of the pillars of our species' thinking.


**2812 (highlight)**

Duplicating the design paradigms of nature will, I believe, be a key trend in future computing.


**2815 (highlight)**

analog computing does have an engineering advantage: it is potentially thousands of times more efficient. An analog computation can be performed by a few transistors or, in the case of mammalian neurons, specific electrochemical processes. A digital computation, in contrast, requires thousands or tens of thousands of transistors. On the other hand, this advantage can be offset by the ease of programming (and modifying) digital computer-based simulations.


**2827 (highlight)**

These two factors (slow cycle time and massive parallelism) result in a certain level of computational capacity for the brain, as we discussed earlier. Today our largest supercomputers are approaching this range.


**2840 (highlight)**

·The brain rewires itself. Dendrites are continually exploring new spines and synapses.


**2843 (highlight)**

Contemporary computers don't literally rewire themselves (although emerging "self-healing systems" are starting to do this), but we can effectively simulate this process in software.15 In the future, we can implement this in hardware,


**2853 (highlight)**

·The brain uses emergent properties. Intelligent behavior is an emergent property of the brain's chaotic and complex activity. Consider the analogy to the apparently intelligent design of termite and ant colonies, with their delicately constructed interconnecting tunnels and ventilation systems. Despite their clever and intricate design, ant and termite hills have no master architects; the architecture emerges from the unpredictable interactions of all the colony members, each following relatively simple rules.


**2865 (highlight)**

The basic learning paradigm used by the brain is an evolutionary one: the patterns of connections that are most successful in making sense of the world and contributing to recognitions and decisions survive. A newborn's brain contains mostly randomly linked interneuronal connections, and only a portion of those survive in the two-year-old brain.17


**2871 (highlight)**

The resulting information is not found in specific nodes or connections but rather is a distributed pattern.


**2875 (highlight)**

The brain gets its resilience from being a deeply connected network in which information has many ways of navigating from one point to another.


**2899 (highlight)**

We are now approaching the knee of the curve (the period of rapid exponential growth) in the accelerating pace of understanding the human brain,


**2912 (highlight)**

It was a student of Adrian, Horace Barlow, who contributed another lasting insight, "trigger features" in neurons, with the discovery that the retinas of frogs and rabbits has single neurons that would trigger on "seeing" specific shapes, directions, or velocities. In other words, perception involves a series of stages, with each layer of neurons recognizing more sophisticated features of the image.


**2937 (highlight)**

The ability of neurons to perform multiplication is important because it allowed the behavior of one network of neurons in the brain to be modulated (influenced) by the results of computations of another network.


**2991 (highlight)**

Neuroscience has not yet had access to sensor technology that would achieve this type of analysis, but that situation is about to change. Our tools for peering into our brains are improving at an exponential pace. The resolution of noninvasive brain-scanning devices is doubling about every twelve months (per unit volume).31


**3059 (highlight)**

By the 2020s nanobot technology will be viable, and brain scanning will be one of its prominent applications.


**3139 (highlight)**

Based on these projections, we can conservatively anticipate the requisite nanobot technology to implement these types of scenarios during the 2020s.


**3140 (highlight)**

Once nanobot-based scanning becomes a reality, we will finally be in the same position that circuit designers are in today: we will be able to place highly sensitive and very high-resolution sensors (in the form of nanobots) at millions or even billions of locations in the brain and thus witness in breathtaking detail living brains in action.


**3175 (highlight)**

Brain reverse-engineering will proceed by iterative refinement of both top-to-bottom and bottom-to-top models and simulations, as we refine each level of description and modeling.


**3182 (highlight)**

cerebellum—demonstrate that building precise mathematical models of our brains and then simulating these models with computation is a challenging but viable task once the data capabilities become available.


**3229 (highlight)**

The most exciting new development in our understanding of the synapse is that the topology of the synapses and the connections they form are continually changing. Our first glimpse into the rapid changes in synaptic connections was revealed by an innovative scanning system that requires a genetically modified animal whose neurons have been engineered to emit a fluorescent green light. The system can image living neural tissue and has a sufficiently high resolution to capture not only the dendrites (interneuronal connections) but the spines: tiny projections that sprout from the dendrites and initiate potential synapses.         Neurobiologist Karel Svoboda and his colleagues at Cold Spring Harbor Laboratory on Long Island used the scanning system on mice to investigate networks of neurons that analyze information from the whiskers, a study that provided a fascinating look at neural learning. The dendrites continually grew new spines. Most of these lasted only a day or two, but on occasion a spine would remain stable. "We believe that the high turnover that we see might play an important role in neural plasticity, in that the sprouting spines reach out to probe different presynaptic partners on neighboring neurons,” said Svoboda. "If a given connection is favorable, that is, reflecting a desirable kind of brain rewiring, then these synapses are stabilized and become more permanent. But most of these synapses are not going in the right direction, and they are retracted."58


**3240 (highlight)**

Another consistent phenomenon that has been observed is that neural responses decrease over time, if a particular stimulus is repeated. This adaptation gives greatest priority to new patterns of stimuli.


**3247 (highlight)**

The reason memories can remain intact even if three quarters of the connections have disappeared is that the coding method used appears to have properties similar to those of a hologram. In a hologram, information is stored in a diffuse pattern throughout an extensive region. If you destroy three quarters of the hologram, the entire image remains intact, although with only one quarter of the resolution. Research by Pentti Kanerva, a neuroscientist at Redwood Neuroscience Institute, supports the idea that memories are dynamically distributed throughout a region of neurons. This explains why older memories persist but nonetheless appear to "fade,"


**3253 (highlight)**

Researchers are also discovering that specific neurons perform special recognition tasks.


**3259 (highlight)**

Neurons (biological or otherwise) are a prime example of what is often called chaotic computing. Each neuron acts in an essentially unpredictable fashion. When an entire network of neurons receives input (from the outside world or from other networks of neurons), the signaling among them appears at first to be frenzied and random. Over time, typically a fraction of a second or so, the chaotic interplay of the neurons dies down and a stable pattern of firing emerges. This pattern represents the "decision" of the neural network. If the neural network is performing a pattern-recognition task (and such tasks constitute the bulk of the activity in the human brain), the emergent pattern represents the appropriate recognition. .


**3268 (highlight)**

Essentially, the biological neurons accepted their electronic peers. This indicates that the chaotic mathematical model of these neurons was reasonably accurate.


**3279 (highlight)**

monkeys' food was placed in such a position that the animals had to dexterously manipulate one finger to obtain it. Brain scans before and after revealed dramatic growth in the interneuronal connections and synapses in the region of the brain responsible for controlling that finger.


**3280 (highlight)**

Brain scans before and after revealed dramatic growth in the interneuronal connections and synapses in the region of the brain responsible for controlling that finger.


**3355 (highlight)**

neuromorphic models and simulations lag only slightly behind the availability of the information on which they are based. The rapid success of turning the detailed data from studies of neurons and the interconnection data from neural scanning into effective models and working simulations belies often-stated skepticism about our inherent capability of understanding our own brains.


**3377 (highlight)**

As I had hypothesized, the problem is not solved by building a mental model of three-dimensional motion. Rather, the problem is collapsed by directly translating the observed movements of the ball into the appropriate movement of the player and changes in the configuration of his arms and legs. Alexandre Pouget of the University of Rochester and Lawrence H. Snyder of Washington University have described mathematical "basis functions" that can represent this direct transformation of perceived movement in the visual field to required movements of the muscles.75


**3381 (highlight)**

Furthermore, analysis of recently developed models of the functioning of the cerebellum demonstrate that our cerebellar neural circuits are indeed capable of learning and then applying the requisite basis functions to implement these sensorimotor transformations.


**3381 (note)**

certaines couples input /output peuvent donc etre inferes sans même considerer les etapes intermédiaires


**3404 (highlight)**

Inputs to the alpha motor neurons do not directly specify the movements of each of these muscles but are coded in a more compact, as yet poorly understood, fashion. The final signals to the muscles are determined at lower levels of the nervous system, specifically in the brain stem and spinal cord.79 Interestingly, this organization is taken to an extreme in the octopus, the central nervous system of which apparently sends very high-level commands to each of its arms (such as "grasp that object and bring it closer"), leaving it up to an independent peripheral nervous system in each arm to carry out the mission.80


**3435 (highlight)**

On account of the uniformity of this large region of the brain and the relative simplicity of its interneuronal wiring, its input-output transformations are relatively well understood, compared to those of other brain regions. Although the relevant equations still require refinement, this bottom-up simulation has proved quite impressive.


**3456 (highlight)**

Watts has implemented his model as real-time computer software which, though a work in progress, illustrates the feasibility of converting neurobiological models and brain connection data into working simulations. The software is not based on reproducing each individual neuron and connection, as is the cerebellum model described above, but rather the transformations performed by each region.


**3476 (highlight)**

We've made enough progress in understanding the coding of visual information that experimental retina implants have been developed and surgically installed in patients.97 However, because of the relative complexity of the visual system, our understanding of the processing of visual information lags behind our knowledge of the auditory regions.


**3508 (highlight)**

"Even though we think we see the world so fully, what we are receiving is really just hints, edges in space and time," says Werblin. "These 12 pictures of the world constitute all the information we will ever have about what's out there, and from these 12 pictures, which are so sparse, we reconstruct the richness of the visual world. I'm curious how nature selected these 12 simple movies and how it can be that they are sufficient to provide us with all the information we seem to need."


**3557 (highlight)**

Because it sits at the top of the neural hierarchy, the part of the brain least well understood is the cerebral cortex. This region, which consists of six thin layers in the outermost areas of the cerebral hemispheres, contains billions of neurons.


**3561 (highlight)**

The cortex is responsible for perception, planning, decision making and most of what we regard as conscious thinking.


**3562 (highlight)**

Our ability to use language, another unique attribute of our species, appears to be located in this region.


**3563 (highlight)**

An intriguing hint about the origin of language and a key evolutionary change that enabled the formation of this distinguishing skill is the observation that only a few primates, including humans and monkeys, are able to use an (actual) mirror to master skills. Theorists Giacomo Rizzolatti and Michael Arbib hypothesized that language emerged from manual gestures (which monkeys—and, of course, humans—are capable of). Performing manual gestures requires the ability to mentally correlate the performance and observation of one's own hand movements.111 Their "mirror system hypothesis" is that the key to the evolution of language is a property called "parity," which is the understanding that the gesture (or utterance) has the same meaning for the party making the gesture as for the party receiving it; that is, the understanding that what you see in a mirror is the same (although reversed left-to-right) as what is seen by someone else watching you. Other animals are unable to understand the image in a mirror in this fashion, and it is believed that they are missing this key ability to deploy parity.


**3576 (highlight)**

the authors cite the single attribution of "recursion" as accounting for the unique language faculty of the human species.113 Recursion is the ability to put together small parts into a larger chunk, and then use that chunk as a part in yet another structure and to continue this process iteratively. In this way, we are able to build the elaborate structures of sentences and paragraphs from a limited set of words.


**3583 (highlight)**

shows that neural activity to initiate an action actually occurs about a third of a second before the brain has made the decision to take the action. The implication, according to Libet, is that the decision is really an illusion, that "consciousness is out of the loop." The cognitive scientist and philosopher Daniel Dennett describes the phenomenon as follows: "The action is originally precipitated in some part of the brain, and off fly the signals to muscles, pausing en route to tell you, the conscious agent, what is going on (but like all good officials letting you, the bumbling president, maintain the illusion that you started it all)."114


**3592 (highlight)**

The most complex capability of the human brain—what I would regard as its cutting edge—is our emotional intelligence. Sitting uneasily at the top of our brain's complex and interconnected hierarchy is our ability to perceive and respond appropriately to emotion, to interact in social situations, to have a moral sense, to get the joke, and to respond emotionally to art and music, among other high-level functions.


**3597 (highlight)**

These recent insights have been the result of our attempts to understand how human brains differ from those of other mammals. The answer is that the differences are slight but critical, and they help us discern how the brain processes emotion and related feelings. One difference is that humans have a larger cortex, reflecting our stronger capability for planning, decision making, and other forms of analytic thinking. Another key distinguishing feature is that emotionally charged situations appear to be handled by special cells called spindle cells, which are found only in humans and some great apes.


**3621 (highlight)**

These findings are consistent with a growing consensus that our emotions are closely linked to areas of the brain that contain maps of the body, a view promoted by Dr. Antonio Damasio at the University of Iowa.116 They are also consistent with the view that a great deal of our thinking is directed toward our bodies: protecting and enhancing them, as well as attending to their myriad needs and desires.


**3630 (highlight)**

Interestingly, spindle cells do not exist in newborn humans but begin to appear only at around the age of four months and increase significantly from ages one to three. Children's ability to deal with moral issues and perceive such higher-level emotions as love develop during this same time period.


**3635 (highlight)**

it is remarkable how few neurons appear to be exclusively involved with these emotions. We have fifty billion neurons in the cerebellum that deal with skill formation, billions in the cortex that perform the transformations for perception and rational planning, but only about eighty thousand spindle cells dealing with high-level emotions.


**3644 (highlight)**

Another important application will be to actually interface our brains with computers, which I believe will become an increasingly intimate merger in the decades ahead.


**3656 (highlight)**

A key challenge in connecting neural implants to biological neurons is that the neurons generate glial cells, which surround a "foreign" object in an attempt to protect the brain. Ted Berger and his colleagues are developing special coatings that will appear to be biological and therefore attract rather than repel nearby neurons.         Another approach being pursued by the Max Planck Institute for Human Cognitive and Brain Sciences in Munich is directly interfacing nerves and electronic devices. A chip created by Infineon allows neurons to grow on a special substrate that provides direct contact between nerves and electronic sensors and stimulators. Similar work on a "neurochip" at Caltech has demonstrated two-way, noninvasive communication between neurons and electronics.117


**3665 (highlight)**

The biological neurons in the vicinity of this FDA-approved brain implant receive signals from the electronic device and respond just as if they had received signals from the biological neurons that were once functional.


**3666 (highlight)**

Recent versions of the Parkinson's-disease implant provide the ability to download upgraded software directly to the implant from outside the patient.


**3669 (highlight)**

Homo sapiens, the first truly free species, is about to decommission natural selection, the force that made us....[S]oon we must look deep within ourselves and decide what we wish to become.


**3697 (highlight)**

We have demonstrated that our ability to build detailed models and working simulations of subcellular portions, neurons, and extensive neural regions follows closely upon the availability of the requisite tools and data.


**3705 (highlight)**

        Once the nanobot era arrives in the 2020s


**3761 (highlight)**

the end of the 2030s is a conservative projection for successful uploading.


**3953 (highlight)**

I wrote there: "Whereas some of my contemporaries may be satisfied to embrace aging gracefully as part of the cycle of life, that is not my view. It may be 'natural,' but I don't see anything positive in losing my mental agility, sensory acuity, physical limberness, sexual desire, or any other human ability. I view disease and death at any age as a calamity, as problems to be overcome."


**3971 (highlight)**

When I was forty, my biological age was around thirty-eight. Although I am now fifty-six, a comprehensive test of my biological aging (measuring various sensory sensitivities, lung capacity, reaction times, memory, and related tests) conducted at Grossman's longevity clinic measured my biological age at forty. 15 Although there is not yet a consensus on how to measure biological age, my scores on these tests matched population norms for this age. So, according to this set of tests, I have not aged very much in the last sixteen years, which is confirmed by the many blood tests I take, as well as the way I feel.         These results are not accidental; I have been very aggressive about reprogramming my biochemistry. I take 250 supplements (pills) a day and receive a half-dozen intravenous therapies each week (basically nutritional supplements delivered directly into my bloodstream, thereby bypassing my GI tract). As a result, the metabolic reactions in my body are completely different than they would otherwise be.16 Approaching this as an engineer, I measure dozens of levels of nutrients (such as vitamins, minerals, and fats), hormones, and metabolic by-products in my blood and other body samples (such as hair and saliva).


**3989 (highlight)**

biotechnology revolution (which we call "bridge two"), which is already in its early stages and will reach its peak in the second decade of this century.         Biotechnology will provide the means to actually change your genes: not just designer babies will be feasible but designer baby boomers. We'll also be able to rejuvenate all of your body's tissues and organs by transforming your skin cells into youthful versions of every other cell type.


**3995 (highlight)**

stopping the aging process by changing the information processes underlying biology


**4002 (highlight)**

We are beginning to understand aging, not as a single inexorable progression but as a group of related processes. Strategies are emerging for fully reversing each of these aging progressions,


**4007 (highlight)**

De Grey believes we'll demonstrate "robustly rejuvenated" mice—mice that are functionally younger than before being treated and with the life extension to prove it—within ten years, and he points out that this achievement will have a dramatic effect on public opinion. Demonstrating that we can reverse the aging process in an animal that shares 99 percent of our genes will profoundly challenge the common wisdom that aging and death are inevitable. Once robust rejuvenation is confirmed in an animal, there will be enormous competitive pressure to translate these results into human therapies, which should appear five to ten years later.


**4016 (highlight)**

able to design drugs to carry out precise missions at the molecular level.


**4017 (highlight)**

With recently developed gene technologies we're on the verge of being able to control how genes express themselves.


**4021 (highlight)**

The therapeutic control of this process can take place outside the cell nucleus, so it is easier to implement than therapies that require access inside it.


**4024 (highlight)**

Many new therapies now in development and testing are based on manipulating them either to turn off the expression of disease-causing genes or to turn on desirable genes that may otherwise not be expressed in a particular type of cell.


**4056 (highlight)**

Somatic Gene Therapy (gene therapy for nonreproductive cells). This is the holy grail of bioengineering, which will enable us to effectively change genes inside the nucleus by "infecting" it with new DNA,


**4169 (highlight)**

Progress in combating all of these sources of aging is moving rapidly in animal models, and translation into human therapies will follow. Evidence from the genome project indicates that no more than a few hundred genes are involved in the aging process. By manipulating these genes, radical life extension has already been achieved in simpler animals. For example, by modifying genes in the C. elegans worm that control its insulin and sex-hormone levels, the lifespan of the test animals was expanded sixfold, to the equivalent of a five-hundred-year lifespan for a human.


**4232 (highlight)**

Even more exciting is the prospect of replacing one's organs and tissues with their "young" replacements without surgery. Introducing cloned, telomere-extended, DNA-corrected cells into an organ will allow them to integrate themselves with the older cells. By repeated treatments of this kind over a period of time, the organ will end up being dominated by the younger cells. We normally replace our own cells on a regular basis anyway, so why not do so with youthful rejuvenated cells rather than telomere-shortened error-filled ones?


**4284 (highlight)**

Well, if you're speaking for yourself, that's fine with 'me. But if you stay biological and don't reprogram your genes, you won't be around for very long to influence the debate.


**4349 (highlight)**

However, as Drexler points out, a nanoscale assembler does not necessarily have to be self-replicating.76 Given the inherent dangers in self-replication, the ethical standards proposed by the Foresight Institute (a think tank founded by Eric Drexler and Christine Peterson) contain prohibitions against unrestricted self-replication, especially in a natural environment.


**4359 (highlight)**

Drexler's Nanosystems provided a number of feasible chemistries for the tip of the robot arm to make it capable of grasping (using appropriate atomic-force fields) a molecular fragment, or even a single atom, and then depositing it in a desired location.


**4375 (highlight)**

Although many configurations have been proposed, the typical assembler has been described as a tabletop unit that can manufacture almost any physically possible product for which we have a software description, ranging from computers, clothes, and works of art to cooked meals.79


**4380 (highlight)**

The incremental cost of creating any physical product, including the assemblers themselves, would be pennies per pound—basically the cost of the raw materials. Drexler estimates total manufacturing cost for a molecular-manufacturing process in the range of ten cents to fifty cents per kilogram, regardless of whether the manufactured product were clothing, massively parallel supercomputers, or additional manufacturing systems.80


**4383 (highlight)**

The real cost, of course, would be the value of the information describing each type of product—that is, the software that controls the assembly process. In other words, the value of everything in the world, including physical objects, would be based essentially on information.


**4391 (highlight)**

In operation, the centralized data store would send out commands simultaneously to many trillions (some estimates as high as 1018) of robots in an assembler, each receiving the same instruction at the same time. The assembler would create these molecular robots by starting with a small number and then using these robots to create additional ones in an iterative fashion, until the requisite number had been created. Each robot would have a local data storage that specifies the type of mechanism it's building. This storage would be used to mask the global instructions being sent from the centralized data store so that certain instructions are blocked and local parameters are filled in. In this way, even though all of the assemblers are receiving the same sequence of instructions, there is a level of customization to the part being built by each molecular robot. This process is analogous to gene expression in biological systems. Although every cell has every gene, only those genes relevant to a particular cell type are expressed. Each robot extracts the raw materials and fuel it needs, which include individual carbon atoms and molecular fragments, from the source material.


**4400 (highlight)**

Nature shows that molecules can serve as machines because living things work by means of such machinery. Enzymes are molecular machines that make, break, and rearrange the bonds holding other molecules together.


**441 (highlight)**

The Age of Spiritual Machines (ASM), which I wrote in 1998, I sought to articulate the nature of human life as it would exist past the point when machine and human cognition blurred.


**4418 (highlight)**

potential to replace biology's genetic-information repository in the cell nucleus with a nanoengineered system that would maintain the genetic code and simulate the actions of RNA, the ribosome, and other elements of the computer in biology's assembler. A nanocomputer would maintain the genetic code and implement the gene-expression algorithms. A nanobot would then construct the amino-acid sequences for the expressed genes.         There would be significant benefits in adopting such a mechanism. We could eliminate the accumulation of DNA transcription errors, one major source of the aging process.


**4424 (highlight)**

We would also be able to defeat biological pathogens (bacteria, viruses, and cancer cells) by blocking any unwanted replication of genetic information.


**4446 (highlight)**

In the decade since publication of Drexler's Nanosystems, each aspect of Drexler's conceptual designs has been validated through additional design proposals.81 supercomputer simulations, and, most important, actual construction of related molecular machines. Boston College chemistry professor T. Ross Kelly reported that he constructed a chemically powered nanomotor out of seventy-eight atoms.82 A biomolecular research group headed by Carlo Montemagno created an ATP-fueled nanomotor.83 Another molecule-sized motor fueled by solar energy was created out of fifty-eight atoms by Ben Feringa at the University of Groningen in the Netherlands.84 Similar progress has been made on other molecular-scale mechanical components such as gears, rotors, and levers. Systems demonstrating the use of chemical energy and acoustic energy (as originally described by Drexler) have been designed, simulated, and actually constructed. Substantial progress has also been made in developing various types of electronic components from molecular-scale devices, particularly in the area of carbon nanotubes, an area that Richard Smalley has pioneered.


**448 (highlight)**

The story is predicated on the idea that we have the ability to understand our own intelligence—to access our own source code, if you will—and then revise and expand it.


**449 (note)**

question is: when will we indeed really understand our source code?


**4516 (highlight)**

Creating this many nanobots at reasonable cost will require self-replication at some level, which while solving the economic issue will introduce potentially grave dangers, a concern I will address in chapter 8. Biology uses the same solution to create organisms with trillions of cells, and indeed we find that virtually all diseases derive from biology's self-replication process gone awry.


**453 (highlight)**

Our progress in reverse engineering the human brain, a key issue that I will describe in detail in this book, demonstrates that we do indeed have the ability to understand, to model, and to extend our own intelligence.


**4579 (highlight)**

Smalley's argument is of the form "We don't have X today, therefore X is impossible." We encounter this class of argument repeatedly in the area of artificial intelligence. Critics will cite the limitations of today's systems as proof that such limitations are inherent and can never be overcome.


**4600 (highlight)**

Like every other technology that humankind has created, it can also be used to amplify and enable our destructive side. It's important that we approach this technology in a knowledgeable manner to gain the profound benefits it promises, while avoiding its dangers.


**4658 (highlight)**

For the past several decades energy technologies have been on the slow slope of the industrial era S-curve (the late stage of a specific technology paradigm, when the capability slowly approaches an asymptote or limit). Although the nanotechnology revolution will require new energy resources, it will also introduce major new S-curves in every aspect of energy—production, storage, transmission, and utilization—by the 2020s.


**482 (highlight)**

This book, then, is the story of the destiny of the human-machine civilization, a destiny we have come to refer to as the Singularity.


**4861 (highlight)**

severing forever the link between calendar time and biological health.


**4873 (highlight)**

Robert A. Freitas Jr.—a


**491 (highlight)**

I've sought to understand the meaning and purpose of the continual upheaval


**4984 (highlight)**

Given that superintelligence will one day be technologically feasible, will people choose to develop it? This question can pretty confidently be answered in the affirmative. Associated with every step along the road to superintelligence are enormous economic payoffs. The computer industry invests huge sums in the next generation of hardware and software, and it will continue doing so as long as there is a competitive pressure and profits to be made. People want better computers and smarter software, and they want the benefits these machines can help produce. Better medical drugs; relief for humans from the need to perform boring or dangerous jobs; entertainment—there is no end to the list of consumer-benefits. There is also a strong military motive to develop artificial intelligence. And nowhere on the path is there any natural stopping point where technophobics could plausibly argue "hither but not further."   —NICK BOSTROM, “HOW LONG BEFORE SUPERINTELLIGENCE?” 1997


**499 (highlight)**

I regard someone who understands the Singularity and who has reflected on its implications for his or her own life as a "singularitarian."1


**5033 (highlight)**

A key question regarding the Singularity is whether the "chicken" (strong AI) or the "egg"(nanotechnology) will come first.


**5040 (highlight)**

Both premises are logical; it's clear that either technology can assist the other. The reality is that progress in both areas will necessarily use our most advanced tools, so advances in each field will simultaneously facilitate the other. However, I do expect that full MNT will emerge prior to strong AI, but only by a few years (around 2025 for nanotechnology, around 2029 for strong AI).


**5049 (highlight)**

The premise is that once strong AI is achieved, it will immediately become a runaway phenomenon of rapidly escalating superintelligence.160


**5094 (highlight)**

We are well into the era of "narrow AI," which refers to artificial intelligence that performs a useful and specific function that once required human intelligence to perform, and does so at human levels or better. Often narrow AI systems greatly exceed the speed of humans, as well as provide the ability to manage and consider thousands of variables simultaneously.


**5128 (highlight)**

it's only recently that we have been able to obtain sufficiently detailed models of how human brain regions function to influence AI design. Prior to that, in the absence of tools that could peer into the brain with sufficient resolution, AI scientists and engineers developed their own techniques.


**5181 (highlight)**

We let the system discover these "rules" for itself from thousands of hours of transcribed human speech data. The advantage of this approach over hand-coded rules is that the models develop subtle probabilistic rules of which human experts are not necessarily aware.


**5185 (highlight)**

This technique involves simulating a simplified model of neurons and interneuronal connections.


**5194 (highlight)**

Since the neural-net wiring and synaptic weights are initially set randomly, the answers of an untrained neural net will be random. The key to a neural net, therefore, is that it must learn its subject matter. Like the mammalian brains on which it's loosely modeled, a neural net starts out ignorant.


**5201 (highlight)**

A powerful, well-taught neural net can emulate a wide range of human pattern-recognition faculties. Systems using multilayer neural nets have shown impressive results in a wide variety of pattern-recognition tasks, including recognizing handwriting, human faces, fraud in commercial transactions such as credit-card charges, and many others.


**5205 (highlight)**

The current trend in neural nets is to take advantage of more realistic and more complex models of how actual biological neural nets work, now that we are developing detailed models of neural functioning


**5207 (highlight)**

Since we do have several decades of experience in using self-organizing paradigms, new insights from brain studies can quickly be adapted to neural-net experiments.


**5212 (highlight)**

Genetic Algorithms (GAs). Another self-organizing paradigm inspired by nature is genetic, or evolutionary, algorithms, which emulate evolution, including sexual reproduction and mutations.


**5214 (highlight)**

If the problem is optimizing the design parameters for a jet engine, define a list of the parameters


**5215 (note)**

un peu comme du test driven development


**5229 (highlight)**

The key to a GA is that the human designers don't directly program a solution; rather, they let one emerge through an iterative process of simulated competition and improvement.


**523 (highlight)**

Although impressive in many respects, the brain suffers from severe limitations.


**5233 (highlight)**

Like neural nets GAs are a way to harness the subtle but profound patterns that exist in chaotic data. A key requirement for their success is a valid way of evaluating each possible solution.


**5236 (highlight)**

GAs are adept at handling problems with too many variables to compute precise analytic solutions. The design of a jet engine, for example, involves more than one hundred variables and requires satisfying dozens of constraints.


**524 (highlight)**

But our thinking is extremely slow:


**5243 (highlight)**

Genetic algorithms, part of the field of chaos or complexity theory, are increasingly being used to solve otherwise intractable business problems, such as optimizing complex supply chains. This approach is beginning to supplant more analytic methods throughout industry.


**528 (highlight)**

While human intelligence is sometimes capable of soaring in its creativity and expressiveness, much human thought is derivative, petty, and circumscribed.


**5288 (highlight)**

The most powerful approach to building robust AI systems is to combine approaches, which is how the human brain works.


**5295 (highlight)**

libraries of recorded and annotated human speech. We then programmed a software "expert manager" to learn the strengths and weaknesses of the different "experts" (recognizers) and combine their results in optimal ways. In this fashion, a particular technique that by itself might produce unreliable results can nonetheless contribute to increasing the overall accuracy of the system.


**5302 (note)**

open cog foundation


**5302 (highlight)**

Ben Goertzel


**5308 (highlight)**

Narrow AI is strengthening as a result of several concurrent trends: continued exponential gains in computational resources, extensive real-world experience with thousands of applications, and fresh insights into how the human brain makes intelligent decisions.


**531 (highlight)**

By the end of this century, the nonbiological portion of our intelligence will be trillions of trillions of times more powerful than unaided human intelligence.         We are now in the early stages of this transition.


**5348 (highlight)**

"We are now using the [GA] software to design tiny microscopic machines, including gyroscopes, for spaceflight navigation. The software also may invent designs that no human designer would ever think of."186


**5360 (highlight)**

Current satellite technology is capable of observing ground-level features about an inch in size and is not affected by bad weather, clouds, or darkness.188


**5363 (highlight)**

My own company (Kurzweil Technologies) is working with United Therapeutics to develop a new generation of automated ECG analysis for long-term unobtrusive monitoring (via sensors embedded in clothing and wireless communication using a cell phone) of the early warning signs of heart disease.189 Other pattern-recognition systems are used to diagnose a variety of imaging data.


**537 (highlight)**

the changes they bring about will appear to rupture the fabric of human history.


**539 (highlight)**

        The Singularity will represent the culmination of the merger of our biological thinking and existence with our technology, resulting in a world that is still human but that transcends our biological roots.


**5391 (highlight)**

A "robot scientist" has been developed at the University of Wales that combines an AI-based system capable of formulating original theories, a robotic system that can automatically carry out experiments, and a reasoning engine to evaluate results. The researchers provided their creation with a model of gene expression in yeast. The system "automatically originates hypotheses to explain observations, devises experiments to test these hypotheses, physically runs the experiments using a laboratory robot, interprets the results to falsify hypotheses inconsistent with the data, and then repeats the cycle."196 The system is capable of improving its performance by learning from its own experience. The experiments designed by the robot scientist were three times less expensive than those designed by human scientists. A test of the machine against a group of human scientists showed that the discoveries made by the machine were comparable to those made by the humans.


**5400 (highlight)**

A long-standing conjecture in algebra was finally proved by an AI system at Argonne National Laboratory. Human mathematicians called the proof "creative."


**541 (highlight)**

If you wonder what will remain unequivocally human in such a world, it's simply this quality: ours is the species that inherently seeks to extend its physical and mental reach beyond current limitations.


**5552 (highlight)**

The range of intelligent tasks in which machines can now compete with human intelligence is continually expanding.


**5586 (highlight)**

it's a conservative projection to expect detailed and realistic models of all brain regions by the late 2020s.


**5601 (highlight)**

we will draw upon the entire range of tools, some derived directly from brain reverse engineering, some merely inspired by what we know about the brain, and some not based on the brain at all but on decades of AI research.


**5606 (highlight)**

With the accumulated knowledge of human civilization increasingly accessible online, future Als will have the opportunity to conduct their education by accessing this vast body of information.


**5623 (highlight)**

In fact, we already have real-time facial animation, and while it is not quite up to these modified Turing standards, it's reasonably close. We also have very natural-sounding voice synthesis, which is often confused with recordings of human speech, although more work is needed on prosodies (intonation). We're likely to achieve satisfactory facial animation and voice production sooner than the Turing-level language and knowledge capabilities.


**5656 (highlight)**

This is why I see the 2030s as a period of consolidation, as machine intelligence rapidly expands its skills and incorporates the vast knowledge bases of our biological human and machine civilization. By the 2040s we will have the opportunity to apply the accumulated knowledge and skills of our civilization to computational platforms that are billions of times more capable than unassisted biological human intelligence.


**5659 (highlight)**

The advent of strong AI is the most important transformation this century will see.


**5705 (highlight)**

It was the fate of bacteria to evolve into a technology-creating species. And it's our destiny now to evolve into the vast intelligence of the Singularity.


**5714 (highlight)**

"Playing God" is actually the highest expression of human nature. The urges to improve ourselves, to master our environment, and to set our children on the best path possible have been the fundamental driving forces of all of human history. Without these urges to "play God," the world as we know it wouldn't exist today. A few million humans would live in savannahs and forests, eeking out a hunter-gatherer existence, without writing or history or mathematics or an appreciation of the intricacies of their own universe and their own inner workings.


**5725 (highlight)**

As the Singularity approaches we will have to reconsider our ideas about the nature of human life and redesign our human institutions. We will explore a few of these ideas and institutions in this chapter.


**5734 (highlight)**

Warfare will move toward nanobot-based weapons, as well as cyberweapons. Learning will first move online, but once our brains are online we will be able to download new knowledge and skills. The role of work will be to create knowledge of all kinds, from music and art to math and science. The role of play will be, well, to create knowledge, so there won't be a clear distinction between work and play.


**5862 (highlight)**

Although artificial hearts are beginning to be feasible replacements, a more effective approach will be to get rid of the heart altogether. Among Freitas's designs are nanorobotic blood cells that provide their own mobility. If the blood moves autonomously, the engineering issues of the extreme pressures required for centralized pumping can be eliminated.


**5870 (highlight)**

With the respirocytes providing greatly improved oxygenation, we will be able to eliminate the lungs by using nanobots to provide oxygen and remove carbon dioxide. As with other systems, we will go through intermediate stages where these technologies simply augment our natural processes, so we can have the best of both worlds. Eventually, though, there will be no reason to continue with the complications of actual breathing and the burdensome requirement of breathable air everywhere we go. If we find breathing itself pleasurable, we can develop virtual ways of having this sensual experience.


**5883 (highlight)**

So What's Left? Let's consider where we are, circa early 2030s. We've eliminated the heart, lungs, red and white blood cells, platelets, pancreas, thyroid and all the hormone-producing organs, kidneys, bladder, liver, lower esophagus, stomach, small intestines, large intestines, and bowel. What we have left at this point is the skeleton, skin, sex organs, sensory organs, mouth and upper esophagus, and brain.


**5891 (highlight)**

the skin, which includes our primary and secondary sex organs, may prove to be an organ we will actually want to keep, or we may at least want to maintain its vital functions of communication and pleasure. However, we will ultimately be able to improve on the skin with new nanoengineered supple materials that will provide greater protection from physical and thermal environmental effects while enhancing our capacity for intimate communication. The same observation holds for the mouth and upper esophagus, which constitute the remaining aspects of the digestive system that we use to experience the act of eating.


**5921 (highlight)**

We Are Becoming Cyborgs. The human body version 2.0 scenario represents the continuation of a long-standing trend in which we grow more intimate with our technology. Computers started out as large, remote machines in air-conditioned rooms tended by white-coated technicians. They moved onto our desks, then under our arms, and now into our pockets. Soon, we'll routinely put them inside our bodies and brains. By the 2030s we will become more nonbiological than biological.


**6034 (highlight)**

We will be able to visit these virtual places and have any kind of interaction with other real, as well as simulated, people (of course, ultimately there won't be a clear distinction between the two), ranging from business negotiations to sensual encounters. "Virtual-reality environment designer" will be a new job description and a new art form.


**6057 (highlight)**

People's identities are frequently closely tied to their bodies ("I'm a person with a big nose," "I'm skinny," "I'm a big guy," and so on). I found the opportunity to become a different person liberating.


**6076 (highlight)**

It is important to point out that well before the end of the first half of the twenty-first century, thinking via nonbiological substrates will predominate.


**6255 (highlight)**

Is death desirable? The "inevitability" of death is deeply ingrained in human thinking. If death seems unavoidable, we have little choice but to rationalize it as necessary, even ennobling.


**631 (highlight)**

Evolution is a process of creating patterns of increasing order. I'll discuss the concept of order in the next chapter; the emphasis in this section is on the concept of patterns.


**6314 (highlight)**

It is this shared species-wide knowledge base that distinguishes us from other animals.


**6331 (highlight)**

what can we conclude about the ultimate longevity of software? The answer is simply this: Information lasts only so long as someone cares about it.


**6335 (highlight)**

The only way that my archive (or any other information base) can remain viable is if it is continually upgraded and ported to the latest hardware and software standards. If an archive remains ignored, it will ultimately become as inaccessible as my old eight-inch PDP-8 floppy disks.


**6337 (note)**

so the integrity of the software is inherently linked to the hardware it is stored on. are westill the same when we upload our brain to a new medium?


**6406 (highlight)**

Extensive research is going into designing swarm intelligence.51 Swarm intelligence describes the way that complex behaviors can arise from large numbers of individual agents, each following relatively simple rules.52 Swarms of insects are often able to devise intelligent solutions to complex problems, such as designing the architecture of a colony, despite the fact that no single member of the swarm possesses the requisite skills.


**6410 (highlight)**

by I-Robot,


**6412 (highlight)**

As robotic systems become physically smaller and larger in number, the principles of self-organizing swarm intelligence will play an increasingly important role.


**6448 (highlight)**

By the late 2030s and 2040s, as we approach human body version 3.0 and the predominance of nonbiological intelligence, the issue of cyberwarfare will move to center stage. When everything is information, the ability to control your own information and disrupt your enemy's communication, command, and control will be a primary determinant of military success.


**648 (highlight)**

Recent theories of physics concerning multiple universes speculate that new universes are created on a regular basis, each with its own unique rules, but that most of these either die out quickly or else continue without the evolution of any interesting patterns (such as Earth-based biology has created) because their rules do not support the evolution of increasingly complex forms.8


**6513 (highlight)**

Given the enormous wealth-creation potential of GNR technologies, we will see the undercIass largely disappear over the next two to three decades (see the discussions of the 2004 World Bank report in chapters 2 and


**6515 (note)**

not so sure about that...


**652 (highlight)**

it's clear that the physical laws of our universe are precisely what they need to be to allow for the evolution of increasing levels of order and complexity.9


**654 (highlight)**

In the second epoch, starting several billion years ago, carbon-based compounds became more and more intricate until complex aggregations of molecules formed self-replicating mechanisms, and life originated.


**6542 (highlight)**

employment in factories and farms has gone from 60 percent to 6 percent in the United States in the past century. Over the next couple of decades, virtually all routine physical and mental work will be automated.


**6549 (highlight)**

Decentralization. The next several decades will see a major trend toward decentralization.


**6558 (highlight)**

But despite apparent controversy, the overwhelming benefits to human health, wealth, expression, creativity, and knowledge quickly become apparent.


**656 (highlight)**

Ultimately, biological systems evolved a precise digital mechanism (DNA) to store information describing a larger society of molecules.


**6578 (highlight)**

on the Intelligent Destiny of the Cosmos: Why We Are Probably Alone in the Universe


**6581 (highlight)**

What is the universe doing questioning itself via one of its smallest products? —D. E. JENKINS, ANGLICAN THEOLOGIAN


**6583 (highlight)**

question....Instead the universe is computing itself. Powered by Standard Model software,


**6585 (highlight)**

As it computes, it maps out its own spacetime geometry to the ultimate precision allowed by the laws of physics. Computation is existence.


**6588 (highlight)**

The more informed recent view is that, even if the likelihood of a star's having a planet with a technology-creating species is very low (for example, one in a million), there are so many stars (that is, billions of trillions of them), that there are bound to be many (billions or trillions) with advanced technology.         This is the view behind SETI—the Search for Extraterrestrial Intelligence—and is the common informed view today. However, there are reasons to doubt the "SETI assumption" that ETI is prevalent.


**660 (highlight)**

For example, in the third epoch, DNA-guided evolution produced organisms that could detect information with their own sensory organs and process and store that information in their own brains and nervous systems.


**6614 (highlight)**

Russian astronomer N. S. Kardashev describes a "type II" civilization as one that has harnessed the power of its star for communication using electromagnetic radiation


**6614 (highlight)**

Russian astronomer N. S. Kardashev describes a "type II" civilization as one that has harnessed the power of its star for communication using electromagnetic radiation (about 4 Î 1026 watts, based on our sun).67 According to my projections (see chapter 3), our civilization will reach that level by the twenty-second century.


**664 (highlight)**

Ultimately, our own species evolved the ability to create abstract mental models of the world we experience and to contemplate the rational implications of these models. We have the ability to redesign the world in our own minds and to put these ideas into action.


**6799 (highlight)**

        These nonbiological sentries would not need to be very large and in fact would primarily comprise information. It is true, however, that just sending information would not be sufficient, for some material-based device that can have a physical impact on other star and planetary systems must be present. However, it would be sufficient for the probes to be self-replicating nanobots (note that a nanobot has nanoscale features but that the overall size of a nanobot is measured in microns).79 We could send swarms of many trillions of them, with some of these "seeds" taking root in another planetary system and then replicating by finding the appropriate materials, such as carbon and other needed elements, and building copies of themselves.


**6807 (highlight)**

Unlike large organisms such as humans, these nanobots, being extremely small, could travel at close to the speed of light.


**6813 (highlight)**

In this way the maximum speed of expansion of a solar system-size intelligence (that is, a type II civilization) into the rest of the universe would be very close to the speed of light.


**6816 (highlight)**

We have to regard the possibility of circumventing the speed of light as speculative, and my projections of the profound changes that our civilization will undergo in this century make no such assumption. However, the potential to engineer around this limit has important implications for the speed with which we will be able to colonize the rest of the universe with our intelligence.


**6822 (highlight)**

Another intriguing suggestion of an action at a distance that appears to occur at speeds far greater than the speed of light is quantum disentanglement. Two particles created together may be "quantum entangled," meaning that while a given property (such as the phase of its spin) is not determined in either particle, the resolution of this ambiguity of the two particles will occur at the same moment. In other words, if the undetermined property is measured in one of the particles, it will also be determined as the exact same value at the same instant in the other particle, even if the two have traveled far apart. There is an appearance of some sort of communication link between the particles.


**6838 (highlight)**

Yet even if we accept the interpretation of these experiments as indicating a quantum link between the two particles, the apparent communication is transmitting only randomness (profound quantum randomness) at speeds far greater than the speed of light, not predetermined information, such as the bits in a file.


**6847 (highlight)**

another faster-than-the-speed-of-light phenomenon is the speed with which galaxies can recede from each other as a result of the expansion of the universe. If the distance between two galaxies is greater than what is called the Hubble distance, then these galaxies are receding from one another at faster than the speed of light.82 This does not violate Einstein's special theory of relativity, because this velocity is caused by space itself expanding rather than the galaxies moving through space.


**6886 (highlight)**

A significant portion of these wormholes is likely to still be around and may be pervasive, providing a vast network of corridors that reach far and wide throughout the universe. It might be easier to discover and use these natural wormholes than to create new ones.


**6896 (highlight)**

consistent with recent theories that it was significantly higher during the inflationary period of the universe (an early phase in its history, when it underwent very rapid expansion). These experiments showing possible variation in the speed of light clearly need corroboration and are showing only small changes. But if confirmed, the findings would be profound, because it is the role of engineering to take a subtle effect and greatly amplify


**6908 (highlight)**

The conclusion I reach is that it is likely (although not certain) that there are no such other civilizations. In other words, we are in the lead. That's right, our humble civilization with its pickup trucks, fast food, and persistent conflicts (and computation!) is in the lead


**6968 (highlight)**

Leonard Susskind, the discoverer of string theory, and Lee Smolin, a theoretical physicist and expert on quantum gravity, have suggested that universes give rise to other universes in a natural, evolutionary process that gradually refines the natural constants. In other words it is not by accident that the rules and constants of our universe are ideal for evolving intelligent life but rather that they themselves evolved to be that way.


**6978 (highlight)**

In The Age of Spiritual Machines, I introduced a related idea—namely, that intelligence would ultimately permeate the universe and would decide the destiny of the cosmos:


**699 (highlight)**

The fifth epoch will enable our human-machine civilization to transcend the human brain's limitations of a mere hundred trillion extremely slow connections.14


**7104 (highlight)**

Even if we are limited to the one universe we already know about, saturating its matter and energy with intelligence is our ultimate fate.


**7137 (highlight)**

Philosophies of life rooted in centuries-old traditions contain much wisdom concerning personal, organizational, and social living. Many of us also find shortcomings in those traditions. How could they not reach some mistaken conclusions when they arose in pre-scientific times? At the same time, ancient philosophies of life have little or nothing to say about fundamental issues confronting us as advanced technologies begin to enable us to change our identity as individuals and as humans and as economic, cultural, and political forces change global relationships.   —MAX MORE, "PRINCIPLES OF EXTROPY"


**7152 (highlight)**

Philosophers have long noted that their children were born into a more complex world than that of their ancestors. This early and perhaps even unconscious recognition of accelerating change may have been the catalyst for much of the utopian, apocalyptic, and millennialist thinking in our Western tradition. But the modern difference is that now everyone notices the pace of progress on some level, not simply the visionaries.


**7166 (highlight)**

The origin of my quest to understand technology trends was practical: an attempt to time my inventions and to make optimal tactical decisions in launching technology enterprises.


**7169 (highlight)**

So, while being a Singularitarian is not a matter of faith but one of understanding, pondering the scientific trends I've discussed in this book inescapably engenders new perspectives on the issues that traditional religions have attempted to address: the nature of mortality and immortality, the purpose of our lives, and intelligence in the universe.


**7174 (highlight)**

that nothing fundamental will change in our lifetimes.


**7175 (note)**

for somereason I have thought for a long time that my lifetime was going to include tremendous changes


**7179 (highlight)**

While it is fundamentally an understanding of basic technology trends, it is simultaneously an insight that causes one to rethink everything, from the nature of health and wealth to the nature of death and self.


**718 (highlight)**

there seems no reason why progress itself would not involve the creation of still more intelligent entities—on a still-shorter time scale. The best analogy that I see is with the evolutionary past: Animals can adapt to problems and make inventions, but often no faster than natural selection can do its work—the world acts as its own simulator in the case of natural selection. We humans have the ability to internalize the world and conduct "what if's" in our heads; we can solve many problems thousands of times faster than natural selection.


**7207 (highlight)**

·A primary role of traditional religion is deathist rationalization—that is, rationalizing the tragedy of death as a good thing.


**721 (highlight)**

Now, by creating the means to execute those simulations at much higher speeds, we are entering a regime as radically different from our human past as we humans are from the lower animals.


**7215 (highlight)**

·Having reached a tipping point, we will within this century be ready to infuse our solar system with our intelligence through self-replicating nonbiological intelligence. It will then spread out to the rest of the universe.


**7225 (highlight)**

We can anticipate the capability of technologies that are coming—in five years or ten years or twenty—and work these into our plans. That is how I design my own technology projects,


**7238 (highlight)**

More also worries about a cultural rebellion "seduced by religious and cultural urgings for 'stability' 'peace' and against 'hubris' and 'the unknown' "that may derail technological acceleration.7 In my view any significant derailment of the overall advancement of technology is unlikely. Even epochal events such as two world wars (in which on the order of one hundred million people died), the cold war, and numerous economic, cultural, and social upheavals have failed to make the slightest dent in the pace of technology trends. But the reflexive, thoughtless antitechnology sentiments increasingly being voiced in the world today do have the potential to exacerbate a lot of suffering.


**7244 (highlight)**

to me being human means being part of a civilization that seeks to extend its boundaries.


**7256 (highlight)**

We'd want to keep two principles: one from traditional religion and one from secular arts and sciences—from traditional religion, the respect for human consciousness.


**7258 (highlight)**

Right, our morality and legal system are based on respect for the consciousness of others.


**7262 (highlight)**

From the arts and sciences, it is the importance of knowledge.


**7502 (highlight)**

my personal philosophy remains based on patternism—I am principally a pattern that persists in time. I am an evolving pattern, and I can influence the course of the evolution of my pattern. Knowledge is a pattern, as distinguished from mere information, and losing knowledge is a profound loss. Thus, losing a person is the ultimate loss.


**7542 (highlight)**

it is precisely in the world of matter and energy that we encounter transcendence, a principal connotation of what people refer to as spirituality.


**7552 (highlight)**

Although I have been called a materialist, I regard myself as a "patternist," It's through the emergent powers of the pattern that we transcend. Since the material stuff of which we are made turns over quickly, it is the transcendent power of our patterns that persists.         The power of patterns to endure goes beyond explicitly self-replicating systems, such as organisms and self-replicating technology. It is the persistence and power of patterns that support life and intelligence. The pattern is far more important than the material stuff that constitutes it.


**7560 (highlight)**

Although some regard what is referred to as "spiritual" as the true meaning of transcendence, transcendence refers to all levels of reality: the creations of the natural world, including ourselves, as well as our own creations in the form of art, culture, technology, and emotional and spiritual expression. Evolution concerns patterns, and it is specifically the depth and order of patterns that grow in an evolutionary process.


**7602 (highlight)**

I meant that people usually mean something more by the word "God" than "just" the material world. Some people do associate God with everything that exists, but they still consider God to be conscious. So you believe in a God that's not conscious?


**7613 (highlight)**

We are being propelled into this new century with no plan, no control, no brakes....The only realistic alternative I see is relinquishment: to limit development of the technologies that are too dangerous, by limiting our pursuit of certain kinds of knowledge.


**762 (highlight)**

the Singularity has many faces. It represents the nearly vertical phase of exponential growth that occurs when the rate is so extreme that technology appears to be expanding at infinite speed. Of course, from a mathematical perspective, there is no discontinuity, no rupture, and the growth rates remain finite, although extraordinarily large. But from our currently limited framework, this imminent event appears to be an acute and abrupt break in the continuity of progress.


**7693 (highlight)**

My discussion of the downsides of future technology alarmed Joy, as he would later relate in his now-famous cover story for Wired, "Why the Future Doesn't Need Us."8 In the article Joy describes how he asked his friends in the scientific and technology community whether the projections I was making were credible and was dismayed to discover how close these capabilities were to realization.


**7706 (highlight)**

venture capitalist with the legendary Silicon Valley firm of Kleiner, Perkins, Caufield & Byers, investing in technologies such as nanotechnology applied to renewable energy and other natural resources,


**784 (highlight)**

·Human brain scanning is one of these exponentially improving technologies. As I will show in chapter 4, the temporal and spatial resolution and bandwidth of brain scanning are doubling each year. We are just now obtaining the tools sufficient to begin serious reverse engineering (decoding) of the human brain's principles of operation.


**787 (highlight)**

Within two decades, we will have a detailed understanding of how all the regions of the human brain work.


**788 (highlight)**

·We will have the requisite hardware to emulate human intelligence with supercomputers by the end of this decade and with personal-computer-size devices by the end of the following decade. We will have effective software models of human intelligence by the mid-2020s.


**8044 (highlight)**

The most challenging issue to resolve is the granularity of relinquishment that is both feasible and desirable.


**8046 (highlight)**

I do think that relinquishment at the right level needs to be part of our ethical response to the dangers of twenty-first-century technologies. One constructive example of this is the ethical guideline proposed by the Foresight Institute:


**8057 (highlight)**

Another good example of a useful ethical guideline is a ban on self-replicating physical entities that contain their own codes for self-replication. In what nanotechnologist Ralph Merkle calls the "broadcast architecture," such entities would have to obtain such codes from a centralized secure server, which would guard against undesirable replication.36


**8076 (highlight)**

As we go forward, balancing our cherished rights of privacy with our need to be protected from the malicious use of powerful twenty-first-century technologies will be one of many profound challenges.


**8142 (highlight)**

I noted above, the software industry is almost completely unregulated. The same is obviously not true for biotechnology. While a bioterrorist does not need to put his "inventions" through the FDA, we do require the scientists developing defensive technologies to follow existing regulations, which slow down the innovation process at every step. Moreover, under existing regulations and ethical standards, it is impossible to test defenses against bioterrorist agents. Extensive discussion is already under way to modify these regulations to allow for animal models and simulations to replace unfeasible human trials. This will be necessary, but I believe we will need to go beyond these steps to accelerate the development of vitally needed defensive technologies.


**8150 (highlight)**

In the medical field, in contrast, extensive regulation slows down innovation, so we cannot have the same confidence with regard to the abuse of biotechnology.


**8155 (highlight)**

This risk-balancing equation will become even more stark when we consider the emerging dangers of bioengineered pathogens. What is needed is a change in public attitude in tolerance for necessary risk.


**8159 (highlight)**

We will not have time to formulate specific countermeasures for each new challenge that comes along. We are close to developing more generalized antiviral technologies, such as RNA interference, and these need to be accelerated.


**8204 (highlight)**

These guidelines and strategies are likely to be effective for preventing accidental release of dangerous self-replicating nanotechnology entities. But dealing with the intentional design and release of such entities is a more complex and challenging problem.


**8281 (highlight)**

·A global program of confidential, random serum monitoring for unknown or evolving biological pathogens should be funded. Diagnostic tools exist to rapidly identify the existence of unknown protein or nucleic acid sequences. Intelligence is key to defense, and such a program could provide invaluable early warning of an impending epidemic. Such a "pathogen sentinel" program has been proposed for many years by public health authorities but has never received adequate funding.


**8469 (highlight)**

years?) However, as this book has extensively argued, we find remarkably precise and predictable exponential trends when assessing the overall effectiveness (as measured by price-performance, bandwidth, and other measures of capability) of information technologies.


**8504 (highlight)**

We could meet all of our projected energy needs of thirty trillion watts in 2030 with solar power if we captured only 0.03 percent (three ten-thousandths) of the sun's energy as it hit the Earth. This will be feasible with extremely inexpensive, lightweight, and efficient nanoengineered solar panels


**8529 (highlight)**

This is the hardware-versus-software challenge, and it is a significant one. Virtual-reality pioneer Jaron Lanier, for example, characterizes my position and that of other so-called cybernetic totalists as, we'll just figure out the software in some unspecified way—a position he refers to as a software "deus ex machina,"2 This ignores, however, the specific and detailed scenario that I've described by which the software of intelligence will be achieved. The reverse engineering of the human brain, an undertaking that is much further along than Lanier and many other observers realize, will expand our AI toolkit to include the self-organizing methods underlying human intelligence.


**8582 (highlight)**

We can already handle levels of software complexity that exceed what is needed to model and simulate the parallel, self-organizing, fractal algorithms that we are discovering in the human brain.


**8604 (highlight)**

The examples above are not anomalies; most computationally intensive "core" algorithms have undergone significant reductions in the number of operations required. Other examples include sorting, searching, autocorrelation (and other statistical methods), and information compression and decompression. Progress has also been made in parallelizing algorithms—that is, breaking a single method into multiple methods that can be performed simultaneously.


**8860 (highlight)**

Early in the twentieth century mathematicians Alfred North Whitehead and Bertrand Russell published their seminal work, Principia Mathematica, which sought to determine axioms that could serve as the basis for all of mathematics.26 However, they were unable to prove conclusively that an axiomatic system that can generate the natural numbers (the positive integers or counting numbers) would not give rise to contradictions. It was assumed that such a proof would be found sooner or later, but in the 1930s a young Czech mathematician, Kurt Gödel, stunned the mathematical world by proving that within such a system there inevitably exist propositions that can be neither proved nor disproved. It was later shown that such unprovable propositions are as common as provable ones. Gödel's incompleteness theorem, which is fundamentally a proof demonstrating that there are definite limits to what logic, mathematics, and by extension computation can do, has been called the most important in all mathematics, and its implications are still being debated.27


**8868 (highlight)**

A similar conclusion was reached by Alan Turing in the context of understanding the nature of computation. When in 1936 Turing presented the Turing machine (described in chapter 2) as a theoretical model of a computer, which continues today to form the basis of modern computational theory, he reported an unexpected discovery similar to Gödel's.28 In his paper that year he described the concept of unsolvable problems—that is, problems that are well defined, with unique answers that can be shown to exist, but that we can also show can never be computed by a Turing machine.


**8880 (highlight)**

Taken together, the works of Turing, Church, and Gödel were the first formal proofs that there are definite limits to what logic, mathematics, and computation can do.


**8885 (highlight)**

The strong interpretation is that problems that are not solvable on a Turing machine cannot be solved by human thought, either.


**8969 (highlight)**

Because we do not understand the brain very well we are constantly tempted to use the latest technology as a model for trying to understand it. In my childhood we were always assured that the brain was a telephone switchboard. ("What else could it be?") I was amused to see that Sherrington, the great British neuroscientist, thought that the brain worked like a telegraph system. Freud often compared the brain to hydraulic and electromagnetic systems. Leibniz compared it to a mill, and I am told some of the ancient Greeks thought the brain functions like a catapult. At present, obviously, the metaphor is the digital computer.


**9257 (highlight)**

In the 2020s we will routinely have nanobots in our bloodstream keeping us healthy and augmenting our mental capabilities.


**9263 (highlight)**

The Unbearable Slowness of Social Institutions. MIT senior research scientist Joel Cutcher-Gershenfeld writes: "Just looking back over the course of the past century and a half, there have been a succession of political regimes where each was the solution to an earlier dilemma, but created new dilemmas in the subsequent era. For example, Tammany Hall and the political patron model were a vast improvement over the dominant system based on landed gentry—many more people were included in the political process. Yet, problems emerged with patronage, which led to the civil service model—a strong solution to the preceding problem by introducing the meritocracy. Then, of course, civil service became the barrier to innovation and we move to reinventing government. And the story continues."43 Gershenfeld is pointing out that social institutions even when innovative in their day become "a drag on innovation."


**9291 (highlight)**

Dembski ascribes "predictability [as] materialism's main virtue" and cites "hollowness [as] its main fault." He goes on to say that "humans have aspirations. We long for freedom, immortality, and the beatific vision. We are restless until we find our rest in God. The problem for the materialist, however, is that these aspirations cannot be redeemed in the coin of matter." He concludes that humans cannot be mere machines because of "the strict absence of extra-material factors from such systems."


**9388 (highlight)**

The patterns are more important than the materials that embody them.


